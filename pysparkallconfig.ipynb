{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOMomA09/cdk9T65y08n0fw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sireshbabu/DataAnalyst/blob/main/pysparkallconfig.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "td4K5MgqIjuV"
      },
      "outputs": [],
      "source": [
        "# Install Java 8\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -f spark-3.5.1-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "EYLrlx-IKi3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and extract Spark 3.5.1\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.1-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "cSh_-uYnKdwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install findspark\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "_80hlcM9KkNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the delta-spark Python package\n",
        "!pip install -q delta-spark==3.2.0"
      ],
      "metadata": {
        "id": "0X6XqMI4KpCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.1-bin-hadoop3\""
      ],
      "metadata": {
        "id": "RJeEEoTBKu-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "GCFOi1x5KzuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from delta import configure_spark_with_delta_pip\n",
        "\n",
        "# Configure SparkSession with Delta Lake and Hive support\n",
        "builder = SparkSession.builder \\\n",
        "    .appName(\"DeltaHiveApp\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
        "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:3.2.0,org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
        "    .enableHiveSupport() # Enable Hive support\n",
        "\n",
        "# The `configure_spark_with_delta_pip` utility ensures all Delta dependencies are included\n",
        "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
        "\n",
        "print(\"SparkSession created with Delta Lake and Hive support.\")\n"
      ],
      "metadata": {
        "id": "yDl0EhSDK4f3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a simple DataFrame\n",
        "data = [(1, \"Apple\", 50), (2, \"Orange\", 75), (3, \"Banana\", 120)]\n",
        "columns = [\"id\", \"name\", \"quantity\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Define the path for the Delta table\n",
        "delta_table_path = \"/content/my_delta_table\""
      ],
      "metadata": {
        "id": "bu6OQvOFNrQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Write the DataFrame to a Delta table\n",
        "# Use `mode(\"overwrite\")` to handle reruns in the same session\n",
        "df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
        "print(\"DataFrame successfully written to Delta table.\")"
      ],
      "metadata": {
        "id": "MlJHMZE0OAg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the Delta table back into a DataFrame\n",
        "delta_df = spark.read.format(\"delta\").load(delta_table_path)"
      ],
      "metadata": {
        "id": "yY6uQk3ROEat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the contents and schema of the new DataFrame\n",
        "print(\"Reading back the Delta table:\")\n",
        "delta_df.show()\n",
        "delta_df.printSchema()"
      ],
      "metadata": {
        "id": "m1VKf0qHOOOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the path is a Delta table (requires the `delta` library)\n",
        "from delta.tables import DeltaTable\n",
        "is_delta = DeltaTable.isDeltaTable(spark, delta_table_path)\n",
        "print(f\"Is the path a Delta table? {is_delta}\")"
      ],
      "metadata": {
        "id": "N0trPWUlOTaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new DataFrame for the Hive table\n",
        "data = [(101, \"New York\"), (102, \"London\"), (103, \"Tokyo\")]\n",
        "columns = [\"emp_id\", \"city\"]\n",
        "hive_df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Define a table name for Hive\n",
        "hive_table_name = \"employee_locations\"\n",
        "\n",
        "# Save the DataFrame as a managed Hive table\n",
        "hive_df.write.mode(\"overwrite\").saveAsTable(hive_table_name)\n",
        "print(f\"\\nDataFrame successfully written to Hive table '{hive_table_name}'.\")\n",
        "\n",
        "# Read the Hive table back into a DataFrame using Spark SQL\n",
        "hive_read_df = spark.sql(f\"SELECT * FROM {hive_table_name}\")\n",
        "\n",
        "# Display the contents and schema of the DataFrame read from Hive\n",
        "print(\"Reading back the Hive table:\")\n",
        "hive_read_df.show()\n",
        "hive_read_df.printSchema()\n",
        "\n",
        "# Verify that Spark can see the table via Hive\n",
        "spark.sql(\"SHOW TABLES\").show()"
      ],
      "metadata": {
        "id": "omkiQqHcOURF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a temporary view from the Delta DataFrame\n",
        "delta_df.createOrReplaceTempView(\"delta_view\")\n",
        "\n",
        "# Perform a join operation between the Delta view and Hive table using SQL\n",
        "print(\"\\nJoining Delta and Hive tables:\")\n",
        "join_df = spark.sql(f\"\"\"\n",
        "    SELECT\n",
        "        t1.id,\n",
        "        t1.name,\n",
        "        t2.city\n",
        "    FROM delta_view t1\n",
        "    JOIN {hive_table_name} t2 ON t1.id = t2.emp_id\n",
        "\"\"\")\n",
        "\n",
        "# Display the joined DataFrame\n",
        "join_df.show()"
      ],
      "metadata": {
        "id": "hE6kaoLNOeEE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}